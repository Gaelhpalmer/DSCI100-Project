{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor variable selection ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward selection (Eforymson 1966; Draper and Smith 1966) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively build up a model by adding one predictor variable at a time, starting one with no predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following 3 steps until you run out of predictors:\n",
    "1. For each unused predictor, add it to the model to form a candidate model.\n",
    "2. Tune all of the candidate models.\n",
    "3. Update the model to be the candidate model with the highest cross-validation accuracy.\n",
    "\n",
    "Select the model that provides the best trade-off between accuracy and simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set-up ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading packages\n",
    "library(repr)\n",
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "# library(themis)\n",
    "set.seed(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  id = \u001b[32mcol_double()\u001b[39m,\n",
      "  gender = \u001b[31mcol_character()\u001b[39m,\n",
      "  age = \u001b[32mcol_double()\u001b[39m,\n",
      "  hypertension = \u001b[32mcol_double()\u001b[39m,\n",
      "  heart_disease = \u001b[32mcol_double()\u001b[39m,\n",
      "  ever_married = \u001b[31mcol_character()\u001b[39m,\n",
      "  work_type = \u001b[31mcol_character()\u001b[39m,\n",
      "  Residence_type = \u001b[31mcol_character()\u001b[39m,\n",
      "  avg_glucose_level = \u001b[32mcol_double()\u001b[39m,\n",
      "  bmi = \u001b[31mcol_character()\u001b[39m,\n",
      "  smoking_status = \u001b[31mcol_character()\u001b[39m,\n",
      "  stroke = \u001b[32mcol_double()\u001b[39m\n",
      ")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'id'</li><li>'gender'</li><li>'age'</li><li>'hypertension'</li><li>'heart_disease'</li><li>'ever_married'</li><li>'work_type'</li><li>'Residence_type'</li><li>'avg_glucose_level'</li><li>'bmi'</li><li>'smoking_status'</li><li>'stroke'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'id'\n",
       "\\item 'gender'\n",
       "\\item 'age'\n",
       "\\item 'hypertension'\n",
       "\\item 'heart\\_disease'\n",
       "\\item 'ever\\_married'\n",
       "\\item 'work\\_type'\n",
       "\\item 'Residence\\_type'\n",
       "\\item 'avg\\_glucose\\_level'\n",
       "\\item 'bmi'\n",
       "\\item 'smoking\\_status'\n",
       "\\item 'stroke'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'id'\n",
       "2. 'gender'\n",
       "3. 'age'\n",
       "4. 'hypertension'\n",
       "5. 'heart_disease'\n",
       "6. 'ever_married'\n",
       "7. 'work_type'\n",
       "8. 'Residence_type'\n",
       "9. 'avg_glucose_level'\n",
       "10. 'bmi'\n",
       "11. 'smoking_status'\n",
       "12. 'stroke'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"id\"                \"gender\"            \"age\"              \n",
       " [4] \"hypertension\"      \"heart_disease\"     \"ever_married\"     \n",
       " [7] \"work_type\"         \"Residence_type\"    \"avg_glucose_level\"\n",
       "[10] \"bmi\"               \"smoking_status\"    \"stroke\"           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading data\n",
    "stroke <- read_csv(\"data/stroke-data.csv\")\n",
    "colnames(stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Problem with `mutate()` input `bmi`.\n",
      "\u001b[34mℹ\u001b[39m NAs introduced by coercion\n",
      "\u001b[34mℹ\u001b[39m Input `bmi` is `as.numeric(bmi)`.”\n",
      "Warning message in mask$eval_all_mutate(dots[[i]]):\n",
      "“NAs introduced by coercion”\n"
     ]
    }
   ],
   "source": [
    "# cleaning and wrangling\n",
    "# stroke_clean <- stroke %>%\n",
    "#     select(gender, age, hypertension, heart_disease, avg_glucose_level, bmi, smoking_status, stroke) %>%\n",
    "#     mutate(gender = as_factor(gender), \n",
    "#            bmi = as.numeric(bmi),\n",
    "#            smoking_status = as_factor(smoking_status),\n",
    "#            stroke = as_factor(stroke)) %>%\n",
    "#     filter(gender != \"Other\") #Other was removed as there was only one occurrence in the dataset\n",
    "\n",
    "# prep and bake data with upsampling, then use it with everything\n",
    "# gonna overfit data, data leakage\n",
    "stroke_clean <- stroke %>%\n",
    "    select(age, hypertension, heart_disease, avg_glucose_level, bmi, stroke) %>%\n",
    "    mutate(stroke = as_factor(stroke),\n",
    "           bmi = as.numeric(bmi)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>age</th><th scope=col>hypertension</th><th scope=col>heart_disease</th><th scope=col>avg_glucose_level</th><th scope=col>bmi</th><th scope=col>stroke</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0.3784336</td><td>-0.3350744</td><td> 4.0915769</td><td>-0.58622986</td><td>        NA</td><td>0</td></tr>\n",
       "\t<tr><td> 1.1708731</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.08416999</td><td> 1.1691436</td><td>0</td></tr>\n",
       "\t<tr><td>-0.6781525</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.45391087</td><td>-0.2754355</td><td>0</td></tr>\n",
       "\t<tr><td>-0.7221769</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.91127435</td><td> 1.5986130</td><td>0</td></tr>\n",
       "\t<tr><td>-0.8542502</td><td>-0.3350744</td><td>-0.2443408</td><td> 0.18223815</td><td> 0.1670482</td><td>0</td></tr>\n",
       "\t<tr><td> 0.2903847</td><td>-0.3350744</td><td>-0.2443408</td><td>-1.07036692</td><td>-0.1583074</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " age & hypertension & heart\\_disease & avg\\_glucose\\_level & bmi & stroke\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t  0.3784336 & -0.3350744 &  4.0915769 & -0.58622986 &         NA & 0\\\\\n",
       "\t  1.1708731 & -0.3350744 & -0.2443408 & -0.08416999 &  1.1691436 & 0\\\\\n",
       "\t -0.6781525 & -0.3350744 & -0.2443408 & -0.45391087 & -0.2754355 & 0\\\\\n",
       "\t -0.7221769 & -0.3350744 & -0.2443408 & -0.91127435 &  1.5986130 & 0\\\\\n",
       "\t -0.8542502 & -0.3350744 & -0.2443408 &  0.18223815 &  0.1670482 & 0\\\\\n",
       "\t  0.2903847 & -0.3350744 & -0.2443408 & -1.07036692 & -0.1583074 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 6\n",
       "\n",
       "| age &lt;dbl&gt; | hypertension &lt;dbl&gt; | heart_disease &lt;dbl&gt; | avg_glucose_level &lt;dbl&gt; | bmi &lt;dbl&gt; | stroke &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "|  0.3784336 | -0.3350744 |  4.0915769 | -0.58622986 |         NA | 0 |\n",
       "|  1.1708731 | -0.3350744 | -0.2443408 | -0.08416999 |  1.1691436 | 0 |\n",
       "| -0.6781525 | -0.3350744 | -0.2443408 | -0.45391087 | -0.2754355 | 0 |\n",
       "| -0.7221769 | -0.3350744 | -0.2443408 | -0.91127435 |  1.5986130 | 0 |\n",
       "| -0.8542502 | -0.3350744 | -0.2443408 |  0.18223815 |  0.1670482 | 0 |\n",
       "|  0.2903847 | -0.3350744 | -0.2443408 | -1.07036692 | -0.1583074 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  age        hypertension heart_disease avg_glucose_level bmi        stroke\n",
       "1  0.3784336 -0.3350744    4.0915769    -0.58622986               NA 0     \n",
       "2  1.1708731 -0.3350744   -0.2443408    -0.08416999        1.1691436 0     \n",
       "3 -0.6781525 -0.3350744   -0.2443408    -0.45391087       -0.2754355 0     \n",
       "4 -0.7221769 -0.3350744   -0.2443408    -0.91127435        1.5986130 0     \n",
       "5 -0.8542502 -0.3350744   -0.2443408     0.18223815        0.1670482 0     \n",
       "6  0.2903847 -0.3350744   -0.2443408    -1.07036692       -0.1583074 0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(stroke_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_split <- initial_split(stroke_clean, prop = 0.75, strata = stroke)\n",
    "stroke_train <- training(stroke_split)\n",
    "stroke_test <- testing(stroke_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_recipe <- recipe(stroke ~ ., data = stroke_train) %>%\n",
    "    step_scale(all_predictors()) %>%\n",
    "    step_center(all_predictors()) %>%\n",
    "    step_downsample(stroke, under_ratio = 1, skip = FALSE) %>%\n",
    "    prep()\n",
    "dsampled_stroke <- bake(ups_recipe, stroke_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`summarise()` ungrouping output (override with `.groups` argument)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 2 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>stroke</th><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0</td><td>3644</td></tr>\n",
       "\t<tr><td>1</td><td>3644</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 2 × 2\n",
       "\\begin{tabular}{ll}\n",
       " stroke & n\\\\\n",
       " <fct> & <int>\\\\\n",
       "\\hline\n",
       "\t 0 & 3644\\\\\n",
       "\t 1 & 3644\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 2 × 2\n",
       "\n",
       "| stroke &lt;fct&gt; | n &lt;int&gt; |\n",
       "|---|---|\n",
       "| 0 | 3644 |\n",
       "| 1 | 3644 |\n",
       "\n"
      ],
      "text/plain": [
       "  stroke n   \n",
       "1 0      3644\n",
       "2 1      3644"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>age</th><th scope=col>hypertension</th><th scope=col>heart_disease</th><th scope=col>avg_glucose_level</th><th scope=col>bmi</th><th scope=col>stroke</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>-1.7787630</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.2474667</td><td>-1.4076731</td><td>0</td></tr>\n",
       "\t<tr><td> 0.6425801</td><td> 2.9836336</td><td>-0.2443408</td><td>-0.4058954</td><td> 1.3513427</td><td>0</td></tr>\n",
       "\t<tr><td>-1.5586409</td><td>-0.3350744</td><td>-0.2443408</td><td> 0.1014749</td><td>-1.4597300</td><td>0</td></tr>\n",
       "\t<tr><td> 1.1708731</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.8245368</td><td> 0.9218733</td><td>0</td></tr>\n",
       "\t<tr><td> 0.3784336</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.6353516</td><td>-1.4467158</td><td>0</td></tr>\n",
       "\t<tr><td> 1.3909952</td><td>-0.3350744</td><td> 4.0915769</td><td> 3.0363899</td><td>-0.2363928</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " age & hypertension & heart\\_disease & avg\\_glucose\\_level & bmi & stroke\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t -1.7787630 & -0.3350744 & -0.2443408 & -0.2474667 & -1.4076731 & 0\\\\\n",
       "\t  0.6425801 &  2.9836336 & -0.2443408 & -0.4058954 &  1.3513427 & 0\\\\\n",
       "\t -1.5586409 & -0.3350744 & -0.2443408 &  0.1014749 & -1.4597300 & 0\\\\\n",
       "\t  1.1708731 & -0.3350744 & -0.2443408 & -0.8245368 &  0.9218733 & 0\\\\\n",
       "\t  0.3784336 & -0.3350744 & -0.2443408 & -0.6353516 & -1.4467158 & 0\\\\\n",
       "\t  1.3909952 & -0.3350744 &  4.0915769 &  3.0363899 & -0.2363928 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 6\n",
       "\n",
       "| age &lt;dbl&gt; | hypertension &lt;dbl&gt; | heart_disease &lt;dbl&gt; | avg_glucose_level &lt;dbl&gt; | bmi &lt;dbl&gt; | stroke &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| -1.7787630 | -0.3350744 | -0.2443408 | -0.2474667 | -1.4076731 | 0 |\n",
       "|  0.6425801 |  2.9836336 | -0.2443408 | -0.4058954 |  1.3513427 | 0 |\n",
       "| -1.5586409 | -0.3350744 | -0.2443408 |  0.1014749 | -1.4597300 | 0 |\n",
       "|  1.1708731 | -0.3350744 | -0.2443408 | -0.8245368 |  0.9218733 | 0 |\n",
       "|  0.3784336 | -0.3350744 | -0.2443408 | -0.6353516 | -1.4467158 | 0 |\n",
       "|  1.3909952 | -0.3350744 |  4.0915769 |  3.0363899 | -0.2363928 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  age        hypertension heart_disease avg_glucose_level bmi        stroke\n",
       "1 -1.7787630 -0.3350744   -0.2443408    -0.2474667        -1.4076731 0     \n",
       "2  0.6425801  2.9836336   -0.2443408    -0.4058954         1.3513427 0     \n",
       "3 -1.5586409 -0.3350744   -0.2443408     0.1014749        -1.4597300 0     \n",
       "4  1.1708731 -0.3350744   -0.2443408    -0.8245368         0.9218733 0     \n",
       "5  0.3784336 -0.3350744   -0.2443408    -0.6353516        -1.4467158 0     \n",
       "6  1.3909952 -0.3350744    4.0915769     3.0363899        -0.2363928 0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>age</th><th scope=col>hypertension</th><th scope=col>heart_disease</th><th scope=col>avg_glucose_level</th><th scope=col>bmi</th><th scope=col>stroke</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.5985557</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.84710628</td><td> 1.1301009</td><td>1</td></tr>\n",
       "\t<tr><td>0.8627022</td><td>-0.3350744</td><td>-0.2443408</td><td> 0.00610785</td><td>-0.6398338</td><td>1</td></tr>\n",
       "\t<tr><td>0.5545313</td><td>-0.3350744</td><td>-0.2443408</td><td> 1.74506268</td><td> 1.5075134</td><td>1</td></tr>\n",
       "\t<tr><td>1.1268487</td><td> 2.9836336</td><td> 4.0915769</td><td>-0.75527955</td><td> 1.0390013</td><td>1</td></tr>\n",
       "\t<tr><td>1.6551417</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.79665689</td><td>-0.3925635</td><td>1</td></tr>\n",
       "\t<tr><td>0.6866045</td><td>-0.3350744</td><td>-0.2443408</td><td>-0.22445468</td><td> 1.9890398</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " age & hypertension & heart\\_disease & avg\\_glucose\\_level & bmi & stroke\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t 0.5985557 & -0.3350744 & -0.2443408 & -0.84710628 &  1.1301009 & 1\\\\\n",
       "\t 0.8627022 & -0.3350744 & -0.2443408 &  0.00610785 & -0.6398338 & 1\\\\\n",
       "\t 0.5545313 & -0.3350744 & -0.2443408 &  1.74506268 &  1.5075134 & 1\\\\\n",
       "\t 1.1268487 &  2.9836336 &  4.0915769 & -0.75527955 &  1.0390013 & 1\\\\\n",
       "\t 1.6551417 & -0.3350744 & -0.2443408 & -0.79665689 & -0.3925635 & 1\\\\\n",
       "\t 0.6866045 & -0.3350744 & -0.2443408 & -0.22445468 &  1.9890398 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 6\n",
       "\n",
       "| age &lt;dbl&gt; | hypertension &lt;dbl&gt; | heart_disease &lt;dbl&gt; | avg_glucose_level &lt;dbl&gt; | bmi &lt;dbl&gt; | stroke &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.5985557 | -0.3350744 | -0.2443408 | -0.84710628 |  1.1301009 | 1 |\n",
       "| 0.8627022 | -0.3350744 | -0.2443408 |  0.00610785 | -0.6398338 | 1 |\n",
       "| 0.5545313 | -0.3350744 | -0.2443408 |  1.74506268 |  1.5075134 | 1 |\n",
       "| 1.1268487 |  2.9836336 |  4.0915769 | -0.75527955 |  1.0390013 | 1 |\n",
       "| 1.6551417 | -0.3350744 | -0.2443408 | -0.79665689 | -0.3925635 | 1 |\n",
       "| 0.6866045 | -0.3350744 | -0.2443408 | -0.22445468 |  1.9890398 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  age       hypertension heart_disease avg_glucose_level bmi        stroke\n",
       "1 0.5985557 -0.3350744   -0.2443408    -0.84710628        1.1301009 1     \n",
       "2 0.8627022 -0.3350744   -0.2443408     0.00610785       -0.6398338 1     \n",
       "3 0.5545313 -0.3350744   -0.2443408     1.74506268        1.5075134 1     \n",
       "4 1.1268487  2.9836336    4.0915769    -0.75527955        1.0390013 1     \n",
       "5 1.6551417 -0.3350744   -0.2443408    -0.79665689       -0.3925635 1     \n",
       "6 0.6866045 -0.3350744   -0.2443408    -0.22445468        1.9890398 1     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsampled_stroke <- bake(ups_recipe, stroke_train)\n",
    "\n",
    "upsampled_stroke %>%\n",
    "  group_by(stroke) %>%\n",
    "  summarize(n = n())\n",
    "\n",
    "head(upsampled_stroke)\n",
    "tail(upsampled_stroke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNSAMPLE here!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'age'</li><li>'avg_glucose_level'</li><li>'bmi'</li><li>'heart_disease'</li><li>'hypertension'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'age'\n",
       "\\item 'avg\\_glucose\\_level'\n",
       "\\item 'bmi'\n",
       "\\item 'heart\\_disease'\n",
       "\\item 'hypertension'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'age'\n",
       "2. 'avg_glucose_level'\n",
       "3. 'bmi'\n",
       "4. 'heart_disease'\n",
       "5. 'hypertension'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"age\"               \"avg_glucose_level\" \"bmi\"              \n",
       "[4] \"heart_disease\"     \"hypertension\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stroke_subset <- upsampled_stroke %>%\n",
    "    filter(!is.na(bmi))\n",
    "names <- colnames(upsampled_stroke %>% select(-stroke)) %>% sort()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'stroke ~ age+avg_glucose_level+bmi+heart_disease+hypertension'"
      ],
      "text/latex": [
       "'stroke \\textasciitilde{} age+avg\\_glucose\\_level+bmi+heart\\_disease+hypertension'"
      ],
      "text/markdown": [
       "'stroke ~ age+avg_glucose_level+bmi+heart_disease+hypertension'"
      ],
      "text/plain": [
       "[1] \"stroke ~ age+avg_glucose_level+bmi+heart_disease+hypertension\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_formula <- paste(\"stroke\", \"~\", paste(names, collapse=\"+\"))\n",
    "example_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 0 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>size</th><th scope=col>model_string</th><th scope=col>accuracy</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 0 × 3\n",
       "\\begin{tabular}{lll}\n",
       " size & model\\_string & accuracy\\\\\n",
       " <int> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 0 × 3\n",
       "\n",
       "| size &lt;int&gt; | model_string &lt;chr&gt; | accuracy &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "\n"
      ],
      "text/plain": [
       "     size model_string accuracy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an empty tibble to store the results\n",
    "accuracies <- tibble(size = integer(), \n",
    "                     model_string = character(), \n",
    "                     accuracy = numeric())\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model specification\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", \n",
    "                             neighbors = tune()) %>%\n",
    "     set_engine(\"kknn\") %>%\n",
    "     set_mode(\"classification\")\n",
    "\n",
    "# create a 5-fold cross-validation object\n",
    "stroke_vfold <- vfold_cv(stroke_subset, v = 5, strata = stroke)\n",
    "\n",
    "gridvals <- tibble(neighbors = seq(from = 1, to = 20, by = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the total number of predictors\n",
    "n_total <- length(names)\n",
    "\n",
    "# stores selected predictors\n",
    "selected <- c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(31)\n",
    "# for every size from 1 to the total number of predictors\n",
    "for (i in 1:n_total) {\n",
    "    print(\"start\")\n",
    "    # for every predictor still not added yet\n",
    "    accs <- list()\n",
    "    models <- list()\n",
    "    for (j in 1:length(names)) {\n",
    "        # create a model string for this combination of predictors\n",
    "        preds_new <- c(selected, names[[j]])\n",
    "        model_string <- paste(\"stroke\", \"~\", paste(preds_new, collapse=\"+\"))\n",
    "        print(model_string)\n",
    "        \n",
    "        # create a recipe from the model string\n",
    "        stroke_recipe <- recipe(as.formula(model_string), \n",
    "                                data = stroke_subset) %>%\n",
    "                          step_scale(all_predictors()) %>%\n",
    "                          step_center(all_predictors())\n",
    "\n",
    "#         !!! how can i repeat the below step more to make it more accurate?\n",
    "        # tune the KNN classifier with these predictors, \n",
    "        # and collect the accuracy for the best K\n",
    "        acc <- workflow() %>%\n",
    "          add_recipe(stroke_recipe) %>%\n",
    "          add_model(knn_spec) %>%\n",
    "          tune_grid(resamples = stroke_vfold, grid = gridvals) %>%\n",
    "          collect_metrics() %>%\n",
    "          filter(.metric == \"accuracy\") %>%\n",
    "          summarize(mx = max(mean))\n",
    "        acc <- acc$mx %>% unlist()\n",
    "\n",
    "        # add this result to the dataframe\n",
    "        accs[[j]] <- acc\n",
    "        models[[j]] <- model_string\n",
    "    }\n",
    "    print(\"unlist\")\n",
    "    print(unlist(accs))\n",
    "    jstar <- which.max(unlist(accs))\n",
    "    accuracies <- accuracies %>% \n",
    "      add_row(size = i, \n",
    "              model_string = models[[jstar]], \n",
    "              accuracy = accs[[jstar]])\n",
    "    selected <- c(selected, names[[jstar]])\n",
    "    names <- names[-jstar]\n",
    "}\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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\nIBH5EJCIfAhIRD4EJCIfAhKRD/0/0waxFpM04fkAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ggplot(accuracies, aes(x=size, y=accuracy)) +\n",
    "geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with predict ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_recipe <- recipe(stroke ~ avg_glucose_level + age, data = stroke_train) %>%\n",
    "  step_scale(all_predictors()) %>%\n",
    "  step_center(all_predictors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_vfold <- vfold_cv(stroke_train, v = 5, strata = stroke)\n",
    "k_vals <- tibble(neighbors = seq(from = 100, to = 200, by = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "  set_engine(\"kknn\") %>%\n",
    "  set_mode(\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results <- workflow() %>%\n",
    "  add_recipe(stroke_recipe) %>%\n",
    "  add_model(knn_spec) %>%\n",
    "  tune_grid(resamples = stroke_vfold, grid = k_vals) %>%\n",
    "  collect_metrics() \n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 92 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>neighbors</th><th scope=col>.metric</th><th scope=col>.estimator</th><th scope=col>mean</th><th scope=col>n</th><th scope=col>std_err</th><th scope=col>.config</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 9</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model009</td></tr>\n",
       "\t<tr><td>10</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model010</td></tr>\n",
       "\t<tr><td>11</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model011</td></tr>\n",
       "\t<tr><td>12</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model012</td></tr>\n",
       "\t<tr><td>13</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model013</td></tr>\n",
       "\t<tr><td>14</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model014</td></tr>\n",
       "\t<tr><td>15</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model015</td></tr>\n",
       "\t<tr><td>16</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model016</td></tr>\n",
       "\t<tr><td>17</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model017</td></tr>\n",
       "\t<tr><td>18</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model018</td></tr>\n",
       "\t<tr><td>19</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model019</td></tr>\n",
       "\t<tr><td>20</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model020</td></tr>\n",
       "\t<tr><td>21</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model021</td></tr>\n",
       "\t<tr><td>22</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model022</td></tr>\n",
       "\t<tr><td>23</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model023</td></tr>\n",
       "\t<tr><td>24</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model024</td></tr>\n",
       "\t<tr><td>25</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model025</td></tr>\n",
       "\t<tr><td>26</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model026</td></tr>\n",
       "\t<tr><td>27</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model027</td></tr>\n",
       "\t<tr><td>28</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model028</td></tr>\n",
       "\t<tr><td>29</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model029</td></tr>\n",
       "\t<tr><td>30</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model030</td></tr>\n",
       "\t<tr><td>31</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model031</td></tr>\n",
       "\t<tr><td>32</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model032</td></tr>\n",
       "\t<tr><td>33</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model033</td></tr>\n",
       "\t<tr><td>34</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model034</td></tr>\n",
       "\t<tr><td>35</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model035</td></tr>\n",
       "\t<tr><td>36</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model036</td></tr>\n",
       "\t<tr><td>37</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model037</td></tr>\n",
       "\t<tr><td>38</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model038</td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td> 71</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model071</td></tr>\n",
       "\t<tr><td> 72</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model072</td></tr>\n",
       "\t<tr><td> 73</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model073</td></tr>\n",
       "\t<tr><td> 74</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model074</td></tr>\n",
       "\t<tr><td> 75</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model075</td></tr>\n",
       "\t<tr><td> 76</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model076</td></tr>\n",
       "\t<tr><td> 77</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model077</td></tr>\n",
       "\t<tr><td> 78</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model078</td></tr>\n",
       "\t<tr><td> 79</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model079</td></tr>\n",
       "\t<tr><td> 80</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model080</td></tr>\n",
       "\t<tr><td> 81</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model081</td></tr>\n",
       "\t<tr><td> 82</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model082</td></tr>\n",
       "\t<tr><td> 83</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model083</td></tr>\n",
       "\t<tr><td> 84</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model084</td></tr>\n",
       "\t<tr><td> 85</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model085</td></tr>\n",
       "\t<tr><td> 86</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model086</td></tr>\n",
       "\t<tr><td> 87</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model087</td></tr>\n",
       "\t<tr><td> 88</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model088</td></tr>\n",
       "\t<tr><td> 89</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model089</td></tr>\n",
       "\t<tr><td> 90</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model090</td></tr>\n",
       "\t<tr><td> 91</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model091</td></tr>\n",
       "\t<tr><td> 92</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model092</td></tr>\n",
       "\t<tr><td> 93</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model093</td></tr>\n",
       "\t<tr><td> 94</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model094</td></tr>\n",
       "\t<tr><td> 95</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model095</td></tr>\n",
       "\t<tr><td> 96</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model096</td></tr>\n",
       "\t<tr><td> 97</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model097</td></tr>\n",
       "\t<tr><td> 98</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model098</td></tr>\n",
       "\t<tr><td> 99</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model099</td></tr>\n",
       "\t<tr><td>100</td><td>accuracy</td><td>binary</td><td>0.9538097</td><td>5</td><td>0.002285071</td><td>Model100</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 92 × 7\n",
       "\\begin{tabular}{lllllll}\n",
       " neighbors & .metric & .estimator & mean & n & std\\_err & .config\\\\\n",
       " <dbl> & <chr> & <chr> & <dbl> & <int> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t  9 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model009\\\\\n",
       "\t 10 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model010\\\\\n",
       "\t 11 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model011\\\\\n",
       "\t 12 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model012\\\\\n",
       "\t 13 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model013\\\\\n",
       "\t 14 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model014\\\\\n",
       "\t 15 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model015\\\\\n",
       "\t 16 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model016\\\\\n",
       "\t 17 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model017\\\\\n",
       "\t 18 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model018\\\\\n",
       "\t 19 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model019\\\\\n",
       "\t 20 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model020\\\\\n",
       "\t 21 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model021\\\\\n",
       "\t 22 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model022\\\\\n",
       "\t 23 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model023\\\\\n",
       "\t 24 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model024\\\\\n",
       "\t 25 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model025\\\\\n",
       "\t 26 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model026\\\\\n",
       "\t 27 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model027\\\\\n",
       "\t 28 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model028\\\\\n",
       "\t 29 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model029\\\\\n",
       "\t 30 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model030\\\\\n",
       "\t 31 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model031\\\\\n",
       "\t 32 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model032\\\\\n",
       "\t 33 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model033\\\\\n",
       "\t 34 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model034\\\\\n",
       "\t 35 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model035\\\\\n",
       "\t 36 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model036\\\\\n",
       "\t 37 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model037\\\\\n",
       "\t 38 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model038\\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t  71 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model071\\\\\n",
       "\t  72 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model072\\\\\n",
       "\t  73 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model073\\\\\n",
       "\t  74 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model074\\\\\n",
       "\t  75 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model075\\\\\n",
       "\t  76 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model076\\\\\n",
       "\t  77 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model077\\\\\n",
       "\t  78 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model078\\\\\n",
       "\t  79 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model079\\\\\n",
       "\t  80 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model080\\\\\n",
       "\t  81 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model081\\\\\n",
       "\t  82 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model082\\\\\n",
       "\t  83 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model083\\\\\n",
       "\t  84 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model084\\\\\n",
       "\t  85 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model085\\\\\n",
       "\t  86 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model086\\\\\n",
       "\t  87 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model087\\\\\n",
       "\t  88 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model088\\\\\n",
       "\t  89 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model089\\\\\n",
       "\t  90 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model090\\\\\n",
       "\t  91 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model091\\\\\n",
       "\t  92 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model092\\\\\n",
       "\t  93 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model093\\\\\n",
       "\t  94 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model094\\\\\n",
       "\t  95 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model095\\\\\n",
       "\t  96 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model096\\\\\n",
       "\t  97 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model097\\\\\n",
       "\t  98 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model098\\\\\n",
       "\t  99 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model099\\\\\n",
       "\t 100 & accuracy & binary & 0.9538097 & 5 & 0.002285071 & Model100\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 92 × 7\n",
       "\n",
       "| neighbors &lt;dbl&gt; | .metric &lt;chr&gt; | .estimator &lt;chr&gt; | mean &lt;dbl&gt; | n &lt;int&gt; | std_err &lt;dbl&gt; | .config &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "|  9 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model009 |\n",
       "| 10 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model010 |\n",
       "| 11 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model011 |\n",
       "| 12 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model012 |\n",
       "| 13 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model013 |\n",
       "| 14 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model014 |\n",
       "| 15 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model015 |\n",
       "| 16 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model016 |\n",
       "| 17 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model017 |\n",
       "| 18 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model018 |\n",
       "| 19 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model019 |\n",
       "| 20 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model020 |\n",
       "| 21 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model021 |\n",
       "| 22 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model022 |\n",
       "| 23 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model023 |\n",
       "| 24 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model024 |\n",
       "| 25 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model025 |\n",
       "| 26 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model026 |\n",
       "| 27 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model027 |\n",
       "| 28 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model028 |\n",
       "| 29 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model029 |\n",
       "| 30 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model030 |\n",
       "| 31 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model031 |\n",
       "| 32 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model032 |\n",
       "| 33 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model033 |\n",
       "| 34 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model034 |\n",
       "| 35 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model035 |\n",
       "| 36 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model036 |\n",
       "| 37 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model037 |\n",
       "| 38 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model038 |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "|  71 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model071 |\n",
       "|  72 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model072 |\n",
       "|  73 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model073 |\n",
       "|  74 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model074 |\n",
       "|  75 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model075 |\n",
       "|  76 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model076 |\n",
       "|  77 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model077 |\n",
       "|  78 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model078 |\n",
       "|  79 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model079 |\n",
       "|  80 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model080 |\n",
       "|  81 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model081 |\n",
       "|  82 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model082 |\n",
       "|  83 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model083 |\n",
       "|  84 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model084 |\n",
       "|  85 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model085 |\n",
       "|  86 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model086 |\n",
       "|  87 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model087 |\n",
       "|  88 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model088 |\n",
       "|  89 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model089 |\n",
       "|  90 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model090 |\n",
       "|  91 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model091 |\n",
       "|  92 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model092 |\n",
       "|  93 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model093 |\n",
       "|  94 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model094 |\n",
       "|  95 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model095 |\n",
       "|  96 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model096 |\n",
       "|  97 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model097 |\n",
       "|  98 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model098 |\n",
       "|  99 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model099 |\n",
       "| 100 | accuracy | binary | 0.9538097 | 5 | 0.002285071 | Model100 |\n",
       "\n"
      ],
      "text/plain": [
       "   neighbors .metric  .estimator mean      n std_err     .config \n",
       "1   9        accuracy binary     0.9538097 5 0.002285071 Model009\n",
       "2  10        accuracy binary     0.9538097 5 0.002285071 Model010\n",
       "3  11        accuracy binary     0.9538097 5 0.002285071 Model011\n",
       "4  12        accuracy binary     0.9538097 5 0.002285071 Model012\n",
       "5  13        accuracy binary     0.9538097 5 0.002285071 Model013\n",
       "6  14        accuracy binary     0.9538097 5 0.002285071 Model014\n",
       "7  15        accuracy binary     0.9538097 5 0.002285071 Model015\n",
       "8  16        accuracy binary     0.9538097 5 0.002285071 Model016\n",
       "9  17        accuracy binary     0.9538097 5 0.002285071 Model017\n",
       "10 18        accuracy binary     0.9538097 5 0.002285071 Model018\n",
       "11 19        accuracy binary     0.9538097 5 0.002285071 Model019\n",
       "12 20        accuracy binary     0.9538097 5 0.002285071 Model020\n",
       "13 21        accuracy binary     0.9538097 5 0.002285071 Model021\n",
       "14 22        accuracy binary     0.9538097 5 0.002285071 Model022\n",
       "15 23        accuracy binary     0.9538097 5 0.002285071 Model023\n",
       "16 24        accuracy binary     0.9538097 5 0.002285071 Model024\n",
       "17 25        accuracy binary     0.9538097 5 0.002285071 Model025\n",
       "18 26        accuracy binary     0.9538097 5 0.002285071 Model026\n",
       "19 27        accuracy binary     0.9538097 5 0.002285071 Model027\n",
       "20 28        accuracy binary     0.9538097 5 0.002285071 Model028\n",
       "21 29        accuracy binary     0.9538097 5 0.002285071 Model029\n",
       "22 30        accuracy binary     0.9538097 5 0.002285071 Model030\n",
       "23 31        accuracy binary     0.9538097 5 0.002285071 Model031\n",
       "24 32        accuracy binary     0.9538097 5 0.002285071 Model032\n",
       "25 33        accuracy binary     0.9538097 5 0.002285071 Model033\n",
       "26 34        accuracy binary     0.9538097 5 0.002285071 Model034\n",
       "27 35        accuracy binary     0.9538097 5 0.002285071 Model035\n",
       "28 36        accuracy binary     0.9538097 5 0.002285071 Model036\n",
       "29 37        accuracy binary     0.9538097 5 0.002285071 Model037\n",
       "30 38        accuracy binary     0.9538097 5 0.002285071 Model038\n",
       "⋮  ⋮         ⋮        ⋮          ⋮         ⋮ ⋮           ⋮       \n",
       "63  71       accuracy binary     0.9538097 5 0.002285071 Model071\n",
       "64  72       accuracy binary     0.9538097 5 0.002285071 Model072\n",
       "65  73       accuracy binary     0.9538097 5 0.002285071 Model073\n",
       "66  74       accuracy binary     0.9538097 5 0.002285071 Model074\n",
       "67  75       accuracy binary     0.9538097 5 0.002285071 Model075\n",
       "68  76       accuracy binary     0.9538097 5 0.002285071 Model076\n",
       "69  77       accuracy binary     0.9538097 5 0.002285071 Model077\n",
       "70  78       accuracy binary     0.9538097 5 0.002285071 Model078\n",
       "71  79       accuracy binary     0.9538097 5 0.002285071 Model079\n",
       "72  80       accuracy binary     0.9538097 5 0.002285071 Model080\n",
       "73  81       accuracy binary     0.9538097 5 0.002285071 Model081\n",
       "74  82       accuracy binary     0.9538097 5 0.002285071 Model082\n",
       "75  83       accuracy binary     0.9538097 5 0.002285071 Model083\n",
       "76  84       accuracy binary     0.9538097 5 0.002285071 Model084\n",
       "77  85       accuracy binary     0.9538097 5 0.002285071 Model085\n",
       "78  86       accuracy binary     0.9538097 5 0.002285071 Model086\n",
       "79  87       accuracy binary     0.9538097 5 0.002285071 Model087\n",
       "80  88       accuracy binary     0.9538097 5 0.002285071 Model088\n",
       "81  89       accuracy binary     0.9538097 5 0.002285071 Model089\n",
       "82  90       accuracy binary     0.9538097 5 0.002285071 Model090\n",
       "83  91       accuracy binary     0.9538097 5 0.002285071 Model091\n",
       "84  92       accuracy binary     0.9538097 5 0.002285071 Model092\n",
       "85  93       accuracy binary     0.9538097 5 0.002285071 Model093\n",
       "86  94       accuracy binary     0.9538097 5 0.002285071 Model094\n",
       "87  95       accuracy binary     0.9538097 5 0.002285071 Model095\n",
       "88  96       accuracy binary     0.9538097 5 0.002285071 Model096\n",
       "89  97       accuracy binary     0.9538097 5 0.002285071 Model097\n",
       "90  98       accuracy binary     0.9538097 5 0.002285071 Model098\n",
       "91  99       accuracy binary     0.9538097 5 0.002285071 Model099\n",
       "92 100       accuracy binary     0.9538097 5 0.002285071 Model100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies <- knn_results %>% # Plot accuracies vs neighbours\n",
    "  filter(.metric == \"accuracy\")\n",
    "optimal_neighbours <- accuracies %>%\n",
    "  filter(mean == max(mean))\n",
    "optimal_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_test_predictions <- predict(knn_fit, stroke_test) %>%\n",
    "  bind_cols(stroke_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stroke_test_predictions %>%\n",
    "  metrics(truth = stroke, estimate = .pred_class) %>%\n",
    "  filter(.metric == \"accuracy\") %>%\n",
    "  arrange(desc(mass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for every size from 1 to the total number of predictors\n",
    "for (i in 1:n_total) {\n",
    "    # for every predictor still not added yet\n",
    "    accs <- list()\n",
    "    models <- list()\n",
    "    for (j in 1:length(names)) {\n",
    "        # create a model string for this combination of predictors\n",
    "        preds_new <- c(selected, names[[j]])\n",
    "        model_string <- paste(\"stroke\", \"~\", paste(preds_new, collapse=\"+\"))\n",
    "        \n",
    "        # create a recipe from the model string\n",
    "        stroke_recipe <- recipe(as.formula(model_string), \n",
    "                                data = stroke_subset) %>%\n",
    "                          step_scale(all_predictors()) %>%\n",
    "                          step_center(all_predictors())\n",
    "\n",
    "        # tune the KNN classifier with these predictors, \n",
    "        # and collect the accuracy for the best K\n",
    "        acc <- workflow() %>%\n",
    "          add_recipe(stroke_recipe) %>%\n",
    "          add_model(knn_spec) %>%\n",
    "          tune_grid(resamples = stroke_vfold, grid = 10) %>%\n",
    "          collect_metrics() %>%\n",
    "          filter(.metric == \"accuracy\") %>%\n",
    "          summarize(mx = max(mean))\n",
    "        acc <- acc$mx %>% unlist()\n",
    "\n",
    "        # add this result to the dataframe\n",
    "        accs[[j]] <- acc\n",
    "        models[[j]] <- model_string\n",
    "    }\n",
    "    jstar <- which.max(unlist(accs))\n",
    "    accuracies <- accuracies %>% \n",
    "      add_row(size = i, \n",
    "              model_string = models[[jstar]], \n",
    "              accuracy = accs[[jstar]])\n",
    "    selected <- c(selected, names[[jstar]])\n",
    "    names <- names[-jstar]\n",
    "}\n",
    "accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
